{
 "metadata": {
  "name": "",
  "signature": "sha256:012a571e5e4c2da0f8331f7a10e572c6110602e2199499996e95b9f23063bab3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import json\n",
      "import zipfile\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv as csv\n",
      "import matplotlib.pyplot as plt\n",
      "import datetime\n",
      "\n",
      "\n",
      "#return a text parser for the large file, with chunk size \"sz\"\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "#remove rows with empty POLYLINE field\n",
      "def rem_empty_polyline(X):\n",
      "    empty_rows=[]\n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(entry==[]):\n",
      "                  empty_rows.append(j)\n",
      "    return X.drop(X.index[empty_rows])\n",
      "    \n",
      "\n",
      "#remove rows with incomplete GPS data (only ten cases)\n",
      "def rem_missing(X):\n",
      "    empty_rows=[]\n",
      "    for j in range(len(X['MISSING_DATA'])):      \n",
      "        entry=X['MISSING_DATA'].values[j]\n",
      "        if(entry==\"True\"):\n",
      "            empty_rows.append(j)\n",
      "    return X.drop(X.index[empty_rows])    \n",
      "    \n",
      "    \n",
      "\n",
      "#add the last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_last(X):\n",
      "\n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        else:\n",
      "            last=entry[-1]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['LAST_LAT']=longitudes \n",
      "    X['LAST_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "#add the first latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_first(X):\n",
      "    \n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        else:\n",
      "            last=entry[0]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['FIRST_LAT']=longitudes \n",
      "    X['FIRST_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# add the second to last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_2ndToLast(X):\n",
      "    \n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        elif(len(entry)==1):\n",
      "            last=entry[-1]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])            \n",
      "        else:\n",
      "            last=entry[-2]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['S2L_LAT']=longitudes \n",
      "    X['S2L_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# truncate the polyline data if the length is >1 using a flat distribution\n",
      "def trunc_polyline(X):\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        m=len(entry)\n",
      "        if m>1:\n",
      "          cut=np.random.randint(1,m+1)\n",
      "          X['POLYLINE'].values[j]=entry[:cut]  \n",
      "\n",
      "    return X\n",
      "\n",
      "\n",
      "#\n",
      "# Evaluation metric -- appears to be off by factor of 2!? no time, figure out later\n",
      "#\n",
      "#  phi_i are latitudes and lambda_j are longitudes\n",
      "#\n",
      "d_2_rad=np.pi/180.0\n",
      "\n",
      "#compute haversine distance between two coordinates (phi_1,lambda_1) and (phi_2,lambda_2)\n",
      "def haversine(phi_1,lambda_1,phi_2,lambda_2):\n",
      "    r=6371  #kilometers\n",
      "    #r=3959 #miles\n",
      "    a= np.sin(d_2_rad*(phi_2-phi_1))**2+np.cos(d_2_rad*phi_1)*np.cos(d_2_rad*phi_2)*np.sin(d_2_rad*(lambda_2-lambda_1))**2\n",
      "    return 2*r*np.arctan(np.sqrt(a/(1-a)))\n",
      "\n",
      "\n",
      "\n",
      "#compute the mean haversine distance between -- not safe, make sure all array dimensions are the same\n",
      "def mean_haversine(phi_1s,lambda_1s,phi_2s,lambda_2s):\n",
      "\n",
      "    total=0\n",
      "    m=len(phi_1s)\n",
      "    for j in range(m):\n",
      "        #print haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
      "        total+=haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
      "\n",
      "    return total/m\n",
      "        \n",
      "\n",
      "    \n",
      "\n",
      "# add the last distance delta from the POLYLINE field to main dataframe X and return it \n",
      "def lastDelta(X):\n",
      "    \n",
      "    deltas=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
      "            deltas.append(-999)\n",
      "        elif(len(entry)==1):          \n",
      "            deltas.append(0)            \n",
      "        else:\n",
      "            last=entry[-1]\n",
      "            Sec2Last=entry[-2]\n",
      "            deltas.append(haversine(last[0],last[1],Sec2Last[0],Sec2Last[1]))\n",
      "            \n",
      "            \n",
      "    X['LDELTA']=deltas \n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "# add the number of points in polyline\n",
      "def num_points(X):\n",
      "       \n",
      "    n_points=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        n_points.append(len(entry))\n",
      "                        \n",
      "    X['NPOINTS']=n_points \n",
      "    \n",
      "    return X\n",
      "    \n",
      "\n",
      "# add the hour of the day \n",
      "def hour(X):\n",
      "    \n",
      "    hour=[]\n",
      "    \n",
      "    for j in range(len(X['TIMESTAMP'])):\n",
      "        entry=X['TIMESTAMP'].values[j]\n",
      "        value=datetime.datetime.fromtimestamp(entry)\n",
      "        daytime_in_hours=float(value.hour)+float(value.minute)/60.0\n",
      "        hour.append(daytime_in_hours)\n",
      "        \n",
      "    X['HOUR']=hour\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# truncate the polyline -- fancy version\n",
      "def trunc_polyline2(X):\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        m=len(entry)\n",
      "       \n",
      "        if m>25:\n",
      "            cut=np.random.randint(1,int(m**1.12)) \n",
      "        else:\n",
      "            cut=np.random.randint(1,int(1.5*m+2))  \n",
      "            \n",
      "        X['POLYLINE'].values[j]=entry[:cut]  \n",
      "\n",
      "    return X\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Train data is order in the file by time of day..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_parser=load_train(100)\n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=hour(chunk)\n",
      "    print chunk.describe()\n",
      "    plt.hist(chunk['HOUR'])\n",
      "    plt.show()\n",
      "    break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "            TRIP_ID   ORIGIN_CALL  ORIGIN_STAND          TAXI_ID  \\\n",
        "count  1.000000e+02     14.000000     19.000000       100.000000   \n",
        "mean   8.136770e+16  30555.214286     26.052632  20000367.860000   \n",
        "std    2.219062e+12  17285.314061     14.210613       209.751753   \n",
        "min    1.372637e+18   2002.000000      7.000000  20000004.000000   \n",
        "25%    1.372638e+18  23097.750000     14.000000  20000193.750000   \n",
        "50%    1.372639e+18  28685.000000     23.000000  20000385.000000   \n",
        "75%    1.372642e+18  37824.000000     34.000000  20000570.000000   \n",
        "max    1.372645e+18  63882.000000     57.000000  20000686.000000   \n",
        "\n",
        "          TIMESTAMP MISSING_DATA        HOUR  \n",
        "count  1.000000e+02          100  100.000000  \n",
        "mean   1.372640e+09            0   20.410000  \n",
        "std    2.219114e+03            0    0.604612  \n",
        "min    1.372637e+09        False   20.000000  \n",
        "25%    1.372638e+09            0   20.000000  \n",
        "50%    1.372639e+09            0   20.000000  \n",
        "75%    1.372642e+09            0   21.000000  \n",
        "max    1.372645e+09        False   22.000000  \n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Try vanilla first and last with daytime in hours\n",
      "\n",
      "Score: 2.94434"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first and last point \n",
      "#\n",
      "#\n",
      "predict_vars = ['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']\n",
      "\n",
      "\n",
      "from sklearn import ensemble \n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "    \n",
      "    \n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=50\n",
      "clf=[] \n",
      "\n",
      "\n",
      "\n",
      "for chunk in train_parser:\n",
      "\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)    \n",
      "    \n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "\n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=hour(chunk)\n",
      "    \n",
      "    chunk=chunk[predict_vars]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15))\n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "        \n",
      "        \n",
      "        \n",
      "\n",
      "        \n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=hour(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        chunk=chunk[predict_vars]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values)\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "  \n",
      "        \n",
      "zft = zipfile.ZipFile(\"../data/test.csv.zip\")\n",
      "test = pd.read_csv(zft.open('test.csv'), chunksize=100, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)}) \n",
      "\n",
      "first_chunk=True\n",
      "\n",
      "for chunk in test:\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=hour(chunk)\n",
      "    \n",
      "    chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']]\n",
      "    \n",
      "    \n",
      "    m=len(clf)\n",
      "    predict=clf[0].predict(chunk.values[:,1:])\n",
      "    for j in range(1,m):  \n",
      "        predict +=clf[j].predict(chunk.values[:,1:])\n",
      "    \n",
      "    predict = predict/float(m)\n",
      "    \n",
      "    submit_df=pd.DataFrame({'TRIP_ID':chunk['TRIP_ID'], 'LONGITUDE':predict[:,1], \n",
      "                            'LATITUDE':predict[:,0]})\n",
      "    if first_chunk:\n",
      "        submit_df.to_csv('../data/7_forest.csv', mode='w', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False)\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        submit_df.to_csv('../data/7_forest.csv', mode='a', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False, header=False)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 1.41632859057\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first and last point \n",
      "#\n",
      "#\n",
      "predict_vars = ['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']\n",
      "\n",
      "\n",
      "from sklearn import ensemble \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train()\n",
      "\n",
      "    \n",
      "    \n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=50\n",
      "clf=[] \n",
      "\n",
      "\n",
      "\n",
      "for chunk in train_parser:\n",
      "\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)    \n",
      "    \n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "\n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=hour(chunk)\n",
      "    \n",
      "    chunk=chunk[predict_vars]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=30))\n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "        \n",
      "        \n",
      "        \n",
      "\n",
      "        \n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train()\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=hour(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        chunk=chunk[predict_vars]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values)\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "  \n",
      "        \n",
      "zft = zipfile.ZipFile(\"../data/test.csv.zip\")\n",
      "test = pd.read_csv(zft.open('test.csv'), chunksize=100, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)}) \n",
      "\n",
      "first_chunk=True\n",
      "\n",
      "for chunk in test:\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=hour(chunk)\n",
      "    \n",
      "    chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']]\n",
      "    \n",
      "    \n",
      "    m=len(clf)\n",
      "    predict=clf[0].predict(chunk.values[:,1:])\n",
      "    for j in range(1,m):  \n",
      "        predict +=clf[j].predict(chunk.values[:,1:])\n",
      "    \n",
      "    predict = predict/float(m)\n",
      "    \n",
      "    submit_df=pd.DataFrame({'TRIP_ID':chunk['TRIP_ID'], 'LONGITUDE':predict[:,1], \n",
      "                            'LATITUDE':predict[:,0]})\n",
      "    if first_chunk:\n",
      "        submit_df.to_csv('../data/8_forest.csv', mode='w', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False)\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        submit_df.to_csv('../data/8_forest.csv', mode='a', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False, header=False)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "CParserError",
       "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mCParserError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-9-9cc6acb4ed19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-8-56c1d99c67b5>\u001b[0m in \u001b[0;36mload_train\u001b[1;34m(sz)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mzf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/train.csv.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'POLYLINE'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m    450\u001b[0m                     infer_datetime_format=infer_datetime_format)\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4628)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._get_header (pandas/parser.c:5921)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas/parser.c:7852)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/home/matt/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:19591)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;31mCParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1) Change truncation scheme based on test data\n",
      "# 2) vector delta, mean delta, median delta?\n",
      "# other parameters ratios of lat/long"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}