{
 "metadata": {
  "name": "",
  "signature": "sha256:bef8674eaa2ef75791eed243be84db6f3cb04c055b84ab29d77e891b26d1deb1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import json\n",
      "import zipfile\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv as csv\n",
      "import matplotlib.pyplot as plt\n",
      "import datetime\n",
      "\n",
      "\n",
      "#return a text parser for the large file, with chunk size \"sz\"\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "#remove rows with empty POLYLINE field\n",
      "def rem_empty_polyline(X):\n",
      "    empty_rows=[]\n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(entry==[]):\n",
      "                  empty_rows.append(j)\n",
      "    return X.drop(X.index[empty_rows])\n",
      "    \n",
      "\n",
      "#remove rows with incomplete GPS data (only ten cases)\n",
      "def rem_missing(X):\n",
      "    empty_rows=[]\n",
      "    for j in range(len(X['MISSING_DATA'])):      \n",
      "        entry=X['MISSING_DATA'].values[j]\n",
      "        if(entry==\"True\"):\n",
      "            empty_rows.append(j)\n",
      "    return X.drop(X.index[empty_rows])    \n",
      "    \n",
      "    \n",
      "\n",
      "#add the last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_last(X):\n",
      "\n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        else:\n",
      "            last=entry[-1]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['LAST_LAT']=longitudes \n",
      "    X['LAST_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "#add the first latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_first(X):\n",
      "    \n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        else:\n",
      "            last=entry[0]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['FIRST_LAT']=longitudes \n",
      "    X['FIRST_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# add the second to last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_2ndToLast(X):\n",
      "    \n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        elif(len(entry)==1):\n",
      "            last=entry[-1]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])            \n",
      "        else:\n",
      "            last=entry[-2]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['S2L_LAT']=longitudes \n",
      "    X['S2L_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# truncate the polyline data if the length is >1 using a flat distribution\n",
      "def trunc_polyline(X):\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        m=len(entry)\n",
      "        if m>1:\n",
      "          cut=np.random.randint(1,m+1)\n",
      "          X['POLYLINE'].values[j]=entry[:cut]  \n",
      "\n",
      "    return X\n",
      "\n",
      "\n",
      "#\n",
      "# Evaluation metric -- appears to be off by factor of 2!? no time, figure out later\n",
      "#\n",
      "#  phi_i are latitudes and lambda_j are longitudes\n",
      "#\n",
      "d_2_rad=np.pi/180.0\n",
      "\n",
      "#compute haversine distance between two coordinates (phi_1,lambda_1) and (phi_2,lambda_2)\n",
      "def haversine(phi_1,lambda_1,phi_2,lambda_2):\n",
      "    r=6371  #kilometers\n",
      "    #r=3959 #miles\n",
      "    a= np.sin(d_2_rad*(phi_2-phi_1))**2+np.cos(d_2_rad*phi_1)*np.cos(d_2_rad*phi_2)*np.sin(d_2_rad*(lambda_2-lambda_1))**2\n",
      "    return 2*r*np.arctan(np.sqrt(a/(1-a)))\n",
      "\n",
      "\n",
      "\n",
      "#compute the mean haversine distance between -- not safe, make sure all array dimensions are the same\n",
      "def mean_haversine(phi_1s,lambda_1s,phi_2s,lambda_2s):\n",
      "\n",
      "    total=0\n",
      "    m=len(phi_1s)\n",
      "    for j in range(m):\n",
      "        #print haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
      "        total+=haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
      "\n",
      "    return total/m\n",
      "        \n",
      "\n",
      "    \n",
      "\n",
      "# add the last distance delta from the POLYLINE field to main dataframe X and return it \n",
      "def lastDelta(X):\n",
      "    \n",
      "    deltas=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
      "            deltas.append(-999)\n",
      "        elif(len(entry)==1):          \n",
      "            deltas.append(0)            \n",
      "        else:\n",
      "            last=entry[-1]\n",
      "            Sec2Last=entry[-2]\n",
      "            deltas.append(haversine(last[0],last[1],Sec2Last[0],Sec2Last[1]))\n",
      "            \n",
      "            \n",
      "    X['LDELTA']=deltas \n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "# add the number of points in polyline\n",
      "def num_points(X):\n",
      "       \n",
      "    n_points=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        n_points.append(len(entry))\n",
      "                        \n",
      "    X['NPOINTS']=n_points \n",
      "    \n",
      "    return X\n",
      "    \n",
      "\n",
      "# add the hour of the day \n",
      "def hour(X):\n",
      "    \n",
      "    hour=[]\n",
      "    \n",
      "    for j in range(len(X['TIMESTAMP'])):\n",
      "        entry=X['TIMESTAMP'].values[j]\n",
      "        value=datetime.datetime.fromtimestamp(entry)\n",
      "        daytime_in_hours=float(value.hour)+float(value.minute)/60.0\n",
      "        hour.append(daytime_in_hours)\n",
      "        \n",
      "    X['HOUR']=hour\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# truncate the polyline -- fancy version\n",
      "def trunc_polyline2(X):\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        m=len(entry)\n",
      "\n",
      "        if m>65:    \n",
      "            a=np.random.uniform()\n",
      "            z=int(1/(a+1.0/m))+1\n",
      "            cut=m-z\n",
      "        else:\n",
      "            cut=min(np.random.geometric(float(1/55.0)),m)\n",
      "\n",
      "\n",
      "        X['POLYLINE'].values[j]=entry[:cut] \n",
      "            \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#mean deltaxy\n",
      "\n",
      "def mean_delta_vec(X):\n",
      "    \n",
      "    x_deltas=[]\n",
      "    y_deltas=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
      "            x_deltas.append(0)\n",
      "            y_deltas.append(0)\n",
      "        elif(len(entry)==1):          \n",
      "            x_deltas.append(0)    \n",
      "            y_deltas.append(0)\n",
      "        else:\n",
      "            last=entry[-1]\n",
      "            first=entry[0]\n",
      "            x_deltas.append((last[0]-first[0]))\n",
      "            y_deltas.append((last[1]-first[1]))\n",
      "            \n",
      "    X['MDLAT']=x_deltas \n",
      "    X['MDLON']=y_deltas\n",
      "        \n",
      "    return X\n",
      "\n",
      "def last_delta_vec(X):\n",
      "    \n",
      "    x_deltas=[]\n",
      "    y_deltas=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
      "            x_deltas.append(0)\n",
      "            y_deltas.append(0)\n",
      "        elif(len(entry)==1):          \n",
      "            x_deltas.append(0)    \n",
      "            y_deltas.append(0)\n",
      "        else:\n",
      "            last=entry[-1]\n",
      "            first=entry[-2]\n",
      "            x_deltas.append((last[0]-first[0]))\n",
      "            y_deltas.append((last[1]-first[1]))\n",
      "            \n",
      "    X['LDLAT']=x_deltas \n",
      "    X['LDLON']=y_deltas\n",
      "        \n",
      "    return X\n",
      "\n",
      "\n",
      "def midpoint_vec(X):\n",
      "    \n",
      "    x_s=[]\n",
      "    y_s=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        m=len(entry)\n",
      "        if m>0:\n",
      "            mid=entry[int(round(m/2.0))-1]\n",
      "            x_s.append(mid[0])\n",
      "            y_s.append(mid[1])\n",
      "        else:\n",
      "            x_s.append(-999)\n",
      "            y_s.append(-999)\n",
      "            \n",
      "    X['MLAT']=x_s \n",
      "    X['MLON']=y_s\n",
      "        \n",
      "    return X\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Baseline values with first and last points: trunc_polyline2:2.0895 truncpolyline:3.943"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Try with mean deltas -- no clear effect\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline2(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=mean_delta_vec(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MDLON','MDLAT']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "      \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        \n",
      "        chunk=trunc_polyline2(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=mean_delta_vec(chunk)        \n",
      "        \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MDLON','MDLAT']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 2.09397107265\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=mean_delta_vec(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MDLON','MDLAT']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=mean_delta_vec(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        \n",
      "        chunk=trunc_polyline(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=mean_delta_vec(chunk)\n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MDLON','MDLAT']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 3.9312676106\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Try Last deltas in vector form -- looks like a good effect"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline2(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=last_delta_vec(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "\n",
      "        chunk=trunc_polyline2(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=last_delta_vec(chunk)  \n",
      "        \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 2.01376307418\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=last_delta_vec(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=last_delta_vec(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        \n",
      "        chunk=trunc_polyline(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=last_delta_vec(chunk)\n",
      "        \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 3.84614399151\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Check delta scalar again for reference -- not as clear cut"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline2(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=lastDelta(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDELTA']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "                \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        \n",
      "        \n",
      "        chunk=trunc_polyline2(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=lastDelta(chunk)\n",
      "        \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDELTA']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 2.00517263286\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=lastDelta(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDELTA']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        \n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        \n",
      "        chunk=trunc_polyline(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=lastDelta(chunk)\n",
      "    \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDELTA']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 3.98815308286\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Look at middle of route point -- not much better"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline2(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=midpoint_vec(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MLON','MLAT']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "\n",
      "        chunk=trunc_polyline2(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=midpoint_vec(chunk)  \n",
      "        \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MLON','MLAT']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 2.08617618905\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try submission 4b with now trunc polyline\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "#from sklearn.neighbors import KNeighborsRegressor \n",
      "\n",
      "\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=5\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "\n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=midpoint_vec(chunk)\n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MLON','MLAT']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "\n",
      "        chunk=trunc_polyline(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=midpoint_vec(chunk)  \n",
      "        \n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','MLON','MLAT']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 3.91892119988\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# looks like last deltas in vector form is the only keeper"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}