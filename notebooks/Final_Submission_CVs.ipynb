{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "#return a text parser for the large file, with chunk size \"sz\"\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "#remove rows with empty POLYLINE field\n",
    "def rem_empty_polyline(X):\n",
    "    empty_rows=[]\n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        if(entry==[]):\n",
    "                  empty_rows.append(j)\n",
    "    return X.drop(X.index[empty_rows])\n",
    "    \n",
    "\n",
    "#remove rows with incomplete GPS data (only ten cases)\n",
    "def rem_missing(X):\n",
    "    empty_rows=[]\n",
    "    for j in range(len(X['MISSING_DATA'])):      \n",
    "        entry=X['MISSING_DATA'].values[j]\n",
    "        if(entry==\"True\"):\n",
    "            empty_rows.append(j)\n",
    "    return X.drop(X.index[empty_rows])    \n",
    "    \n",
    "    \n",
    "\n",
    "#add the last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
    "def lat_long_last(X):\n",
    "\n",
    "    latitudes=[]\n",
    "    longitudes=[]\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        if(len(entry)==0):\n",
    "            latitudes.append(-999)\n",
    "            longitudes.append(-999)\n",
    "        else:\n",
    "            last=entry[-1]           \n",
    "            latitudes.append(last[0])\n",
    "            longitudes.append(last[1])\n",
    "            \n",
    "    X['LAST_LAT']=longitudes \n",
    "    X['LAST_LON']=latitudes\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "#add the first latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
    "def lat_long_first(X):\n",
    "    \n",
    "    latitudes=[]\n",
    "    longitudes=[]\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        if(len(entry)==0):\n",
    "            latitudes.append(-999)\n",
    "            longitudes.append(-999)\n",
    "        else:\n",
    "            last=entry[0]           \n",
    "            latitudes.append(last[0])\n",
    "            longitudes.append(last[1])\n",
    "            \n",
    "    X['FIRST_LAT']=longitudes \n",
    "    X['FIRST_LON']=latitudes\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# add the second to last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
    "def lat_long_2ndToLast(X):\n",
    "    \n",
    "    latitudes=[]\n",
    "    longitudes=[]\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        if(len(entry)==0):\n",
    "            latitudes.append(-999)\n",
    "            longitudes.append(-999)\n",
    "        elif(len(entry)==1):\n",
    "            last=entry[-1]           \n",
    "            latitudes.append(last[0])\n",
    "            longitudes.append(last[1])            \n",
    "        else:\n",
    "            last=entry[-2]           \n",
    "            latitudes.append(last[0])\n",
    "            longitudes.append(last[1])\n",
    "            \n",
    "    X['S2L_LAT']=longitudes \n",
    "    X['S2L_LON']=latitudes\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# truncate the polyline data if the length is >1 using a flat distribution\n",
    "def trunc_polyline(X):\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        m=len(entry)\n",
    "        if m>1:\n",
    "          cut=np.random.randint(1,m+1)\n",
    "          X['POLYLINE'].values[j]=entry[:cut]  \n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "#\n",
    "# Evaluation metric -- appears to be off by factor of 2!? no time, figure out later\n",
    "#\n",
    "#  phi_i are latitudes and lambda_j are longitudes\n",
    "#\n",
    "d_2_rad=np.pi/180.0\n",
    "\n",
    "#compute haversine distance between two coordinates (phi_1,lambda_1) and (phi_2,lambda_2)\n",
    "def haversine(phi_1,lambda_1,phi_2,lambda_2):\n",
    "    r=6371  #kilometers\n",
    "    #r=3959 #miles\n",
    "    a= np.sin(d_2_rad*(phi_2-phi_1))**2+np.cos(d_2_rad*phi_1)*np.cos(d_2_rad*phi_2)*np.sin(d_2_rad*(lambda_2-lambda_1))**2\n",
    "    return 2*r*np.arctan(np.sqrt(a/(1-a)))\n",
    "\n",
    "\n",
    "\n",
    "#compute the mean haversine distance between -- not safe, make sure all array dimensions are the same\n",
    "def mean_haversine(phi_1s,lambda_1s,phi_2s,lambda_2s):\n",
    "\n",
    "    total=0\n",
    "    m=len(phi_1s)\n",
    "    for j in range(m):\n",
    "        #print haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
    "        total+=haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
    "\n",
    "    return total/m\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# add the last distance delta from the POLYLINE field to main dataframe X and return it \n",
    "def lastDelta(X):\n",
    "    \n",
    "    deltas=[]\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
    "            deltas.append(-999)\n",
    "        elif(len(entry)==1):          \n",
    "            deltas.append(0)            \n",
    "        else:\n",
    "            last=entry[-1]\n",
    "            Sec2Last=entry[-2]\n",
    "            deltas.append(haversine(last[0],last[1],Sec2Last[0],Sec2Last[1]))\n",
    "            \n",
    "            \n",
    "    X['LDELTA']=deltas \n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "# add the number of points in polyline\n",
    "def num_points(X):\n",
    "       \n",
    "    n_points=[]\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        n_points.append(len(entry))\n",
    "                        \n",
    "    X['NPOINTS']=n_points \n",
    "    \n",
    "    return X\n",
    "    \n",
    "\n",
    "# add the hour of the day \n",
    "def hour(X):\n",
    "    \n",
    "    hour=[]\n",
    "    \n",
    "    for j in range(len(X['TIMESTAMP'])):\n",
    "        entry=X['TIMESTAMP'].values[j]\n",
    "        value=datetime.datetime.fromtimestamp(entry)\n",
    "        daytime_in_hours=float(value.hour)+float(value.minute)/60.0\n",
    "        hour.append(daytime_in_hours)\n",
    "        \n",
    "    X['HOUR']=hour\n",
    "    \n",
    "    return X\n",
    "\n",
    "def hour2(X):\n",
    "    \n",
    "    hour=[]\n",
    "    \n",
    "    for j in range(len(X['TIMESTAMP'])):\n",
    "        entry=X['TIMESTAMP'].values[j]\n",
    "        value=datetime.datetime.fromtimestamp(entry)\n",
    "        daytime_in_hours=float(value.hour)+float(value.minute)/60.0\n",
    "        if( (daytime_in_hours>5) and (daytime_in_hours<10)):\n",
    "            hour.append('M')\n",
    "        elif( (daytime_in_hours>=10) and (daytime_in_hours<18)):\n",
    "            hour.append('D')\n",
    "        else:\n",
    "            hour.append('E')\n",
    "            \n",
    "    X['HOUR']=hour\n",
    "    X['HOUR']=pd.factorize(X['HOUR'])[0]\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def last_delta_vec(X):\n",
    "    \n",
    "    x_deltas=[]\n",
    "    y_deltas=[]\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
    "            x_deltas.append(0)\n",
    "            y_deltas.append(0)\n",
    "        elif(len(entry)==1):          \n",
    "            x_deltas.append(0)    \n",
    "            y_deltas.append(0)\n",
    "        else:\n",
    "            last=entry[-1]\n",
    "            first=entry[-2]\n",
    "            x_deltas.append((last[0]-first[0]))\n",
    "            y_deltas.append((last[1]-first[1]))\n",
    "            \n",
    "    X['LDLAT']=x_deltas \n",
    "    X['LDLON']=y_deltas\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "# truncate the polyline -- fancy version\n",
    "def trunc_polyline2(X):\n",
    "    \n",
    "    for j in range(len(X['POLYLINE'])):      \n",
    "        entry=X['POLYLINE'].values[j]\n",
    "        m=len(entry)\n",
    "\n",
    "        if m>65:    \n",
    "            a=np.random.uniform()\n",
    "            z=int(1/(a+1.0/m))+1\n",
    "            cut=m-z\n",
    "        else:\n",
    "            cut=min(np.random.geometric(float(1/55.0)),m)\n",
    "            \n",
    "            \n",
    "        X['POLYLINE'].values[j]=entry[:cut] \n",
    "            \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last_delta_vec, DAY_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 1.97029968837\n"
     ]
    }
   ],
   "source": [
    "# last_delta_vec with DAY_TYPE\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "\n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=last_delta_vec(chunk)\n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "\n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=last_delta_vec(chunk)  \n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.84621153721\n"
     ]
    }
   ],
   "source": [
    "# last_delta_vec with DAY_TYPE\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "\n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=last_delta_vec(chunk)\n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "\n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=last_delta_vec(chunk)  \n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#last_delta_vec, DAY_TYPE, HOUR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 1.95683072215\n"
     ]
    }
   ],
   "source": [
    "# last_delta_vec with DAY_TYPE\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "\n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=last_delta_vec(chunk)\n",
    "    chunk=hour2(chunk)\n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "\n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=last_delta_vec(chunk)  \n",
    "        chunk=hour2(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.8014411348\n"
     ]
    }
   ],
   "source": [
    "# last_delta_vec with DAY_TYPE\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "\n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=last_delta_vec(chunk)\n",
    "    chunk=hour2(chunk)\n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "\n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=last_delta_vec(chunk)  \n",
    "        chunk=hour2(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_delta_vec, DAY_TYPE, HOUR2, ORIGIN_CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 1.95871438089\n"
     ]
    }
   ],
   "source": [
    "# last_delta_vec with DAY_TYPE\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "\n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=last_delta_vec(chunk)\n",
    "    chunk=hour2(chunk)\n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR','ORIGIN_CALL']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "\n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=last_delta_vec(chunk)  \n",
    "        chunk=hour2(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR','ORIGIN_CALL']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "        chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.86172200457\n"
     ]
    }
   ],
   "source": [
    "# last_delta_vec with DAY_TYPE\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "\n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=last_delta_vec(chunk)\n",
    "    chunk=hour2(chunk)\n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR','ORIGIN_CALL']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "\n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=last_delta_vec(chunk)  \n",
    "        chunk=hour2(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDLON','LDLAT','DAY_TYPE','HOUR','ORIGIN_CALL']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "        chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Benchmarks w/first and last points\n",
    "## trunc_polyline2: 2.08957296707\n",
    "## trunc_polyline: 3.94312537509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 2.08957296707\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
    "\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "    \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.96205150401\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
    "\n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=30)) #metrics: hamming\n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
    "        \n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "single addition checks below this point.\n",
    "\n",
    "#From other file last_delta_vec scores were: 2.01376307418, 3.84614399151\n",
    "\n",
    "\n",
    "#Check Origin Call -- Slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 2.07101290615\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_CALL']]\n",
    "    chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        \n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "    \n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_CALL']]\n",
    "        chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.93235898506\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_CALL']]\n",
    "    chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_CALL']]\n",
    "        chunk['ORIGIN_CALL']=pd.factorize(chunk['ORIGIN_CALL'])[0]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check ORIGIN_STAND -- small gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 2.07508365069\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_STAND']]\n",
    "    chunk['ORIGIN_STAND']=pd.factorize(chunk['ORIGIN_STAND'])[0]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "    \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_STAND']]\n",
    "        chunk['ORIGIN_STAND']=pd.factorize(chunk['ORIGIN_STAND'])[0]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.95064365365\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_STAND']]\n",
    "    chunk['ORIGIN_STAND']=pd.factorize(chunk['ORIGIN_STAND'])[0]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "    \n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','ORIGIN_STAND']]\n",
    "        chunk['ORIGIN_STAND']=pd.factorize(chunk['ORIGIN_STAND'])[0]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check DAY_TYPE  -- looks like clear improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 2.02382319141\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','DAY_TYPE']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        \n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','DAY_TYPE']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.90254252862\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','DAY_TYPE']]\n",
    "    chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','DAY_TYPE']]\n",
    "        chunk['DAY_TYPE']=pd.factorize(chunk['DAY_TYPE'])[0]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try fuzzy day -- morning, noon, and evening -- slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 2.06344733279\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline2(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=hour2(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=hour2(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline2(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg cv_error: 3.95716757853\n"
     ]
    }
   ],
   "source": [
    "# try submission 4b with now trunc polyline\n",
    "#\n",
    "\n",
    "from sklearn import ensemble \n",
    "#from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "\n",
    "def load_train(sz=100):\n",
    "    zf = zipfile.ZipFile(\"../data/mini_train.csv.zip\")\n",
    "    return pd.read_csv(zf.open('mini_train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
    "\n",
    "\n",
    "\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "\n",
    "target=pd.DataFrame()\n",
    "train_predictors=pd.DataFrame()\n",
    "\n",
    "first_chunk=True\n",
    "k=0\n",
    "cutoff=5\n",
    "clf=[] \n",
    "\n",
    "for chunk in train_parser:\n",
    "    chunk=chunk.fillna(-1)\n",
    "    chunk=rem_empty_polyline(chunk)    \n",
    "    chunk=rem_missing(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    \n",
    "    if(first_chunk):\n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "    else:\n",
    "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
    "        \n",
    "    #\n",
    "    # MODIFY by randomly truncating POLYLINE for training\n",
    "    #  and recompute predictor features\n",
    "    #\n",
    "    chunk=trunc_polyline(chunk)\n",
    "    chunk=lat_long_last(chunk)\n",
    "    chunk=lat_long_first(chunk)\n",
    "    chunk=hour2(chunk)\n",
    "    \n",
    "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']]\n",
    "    \n",
    "    if(first_chunk):\n",
    "        train_predictors=chunk\n",
    "        first_chunk=False\n",
    "    else:\n",
    "        train_predictors=pd.concat([train_predictors,chunk])\n",
    "    \n",
    "    k+=1\n",
    "    if(k==cutoff):\n",
    "        clf.append(ensemble.RandomForestRegressor(n_estimators=15)) \n",
    "        m=len(clf)-1\n",
    "        clf[m].fit(train_predictors.values,target.values)\n",
    "        first_chunk=True\n",
    "        k=0\n",
    "\n",
    "## \n",
    "## Compute ~50/50 CV error\n",
    "##\n",
    "l=0\n",
    "m=len(clf)\n",
    "cut=round(float(m)/2)\n",
    "cv_errors=[]\n",
    "cv_chunks=50\n",
    "\n",
    "# re-initialize the train data parser\n",
    "train_parser=load_train(1000)\n",
    "\n",
    "\n",
    "if(m<2):\n",
    "    print \"only one segment, cannot compute CV error\"\n",
    "else:\n",
    "    \n",
    "    for chunk in train_parser:\n",
    "        chunk=chunk.fillna(-1)\n",
    "        chunk=rem_empty_polyline(chunk)    \n",
    "        chunk=rem_missing(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=hour2(chunk)\n",
    "        \n",
    "        target=chunk[['LAST_LAT','LAST_LON']]\n",
    "        \n",
    "        chunk=trunc_polyline(chunk)\n",
    "        chunk=lat_long_last(chunk)\n",
    "        chunk=lat_long_first(chunk)\n",
    "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','HOUR']]\n",
    "\n",
    "        chunks_models=cut*cutoff\n",
    "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
    "         \n",
    "            predict=0\n",
    "            for j in range(1,int(cut)):  \n",
    "                predict +=clf[j].predict(chunk.values[:,1:])\n",
    "            \n",
    "            predict = predict/float(cut-1)\n",
    "                \n",
    "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
    "            cv_errors.append(mean_dist)\n",
    "        l+=1\n",
    "        \n",
    "    #print \"cv_errors: \" + str(cv_errors)\n",
    "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
    "\n",
    "#\n",
    "# END of CV calculation\n",
    "#\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try In order of Goodness: DAY_TYPE,  ORIGIN_CALL, HOUR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
