{
 "metadata": {
  "name": "",
  "signature": "sha256:41c556983cf5ca109677ed1189d4183a0ed0a0a374b05f5e22ee12032ab9c59f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import json\n",
      "import zipfile\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv as csv\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "#return a text parser for the large file, with chunk size \"sz\"\n",
      "def load_train(sz=100):\n",
      "    zf = zipfile.ZipFile(\"../data/train.csv.zip\")\n",
      "    return pd.read_csv(zf.open('train.csv'), chunksize=sz, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)})\n",
      "\n",
      "\n",
      "\n",
      "#remove rows with empty POLYLINE field\n",
      "def rem_empty_polyline(X):\n",
      "    empty_rows=[]\n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(entry==[]):\n",
      "                  empty_rows.append(j)\n",
      "    return X.drop(X.index[empty_rows])\n",
      "    \n",
      "\n",
      "#remove rows with incomplete GPS data (only ten cases)\n",
      "def rem_missing(X):\n",
      "    empty_rows=[]\n",
      "    for j in range(len(X['MISSING_DATA'])):      \n",
      "        entry=X['MISSING_DATA'].values[j]\n",
      "        if(entry==\"True\"):\n",
      "            empty_rows.append(j)\n",
      "    return X.drop(X.index[empty_rows])    \n",
      "    \n",
      "    \n",
      "\n",
      "#add the last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_last(X):\n",
      "\n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        else:\n",
      "            last=entry[-1]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['LAST_LAT']=longitudes \n",
      "    X['LAST_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "#add the first latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_first(X):\n",
      "    \n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        else:\n",
      "            last=entry[0]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['FIRST_LAT']=longitudes \n",
      "    X['FIRST_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# add the second to last latitude and longitude from the POLYLINE field to main dataframe X and return it \n",
      "def lat_long_2ndToLast(X):\n",
      "    \n",
      "    latitudes=[]\n",
      "    longitudes=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):\n",
      "            latitudes.append(-999)\n",
      "            longitudes.append(-999)\n",
      "        elif(len(entry)==1):\n",
      "            last=entry[-1]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])            \n",
      "        else:\n",
      "            last=entry[-2]           \n",
      "            latitudes.append(last[0])\n",
      "            longitudes.append(last[1])\n",
      "            \n",
      "    X['S2L_LAT']=longitudes \n",
      "    X['S2L_LON']=latitudes\n",
      "    \n",
      "    return X\n",
      "\n",
      "\n",
      "\n",
      "# truncate the polyline data if the length is >1 using a flat distribution\n",
      "def trunc_polyline(X):\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        m=len(entry)\n",
      "        if m>1:\n",
      "          cut=np.random.randint(1,m+1)\n",
      "          X['POLYLINE'].values[j]=entry[:cut]  \n",
      "\n",
      "    return X\n",
      "\n",
      "\n",
      "#\n",
      "# Evaluation metric -- appears to be off by factor of 2!? no time, figure out later\n",
      "#\n",
      "#  phi_i are latitudes and lambda_j are longitudes\n",
      "#\n",
      "d_2_rad=np.pi/180.0\n",
      "\n",
      "#compute haversine distance between two coordinates (phi_1,lambda_1) and (phi_2,lambda_2)\n",
      "def haversine(phi_1,lambda_1,phi_2,lambda_2):\n",
      "    r=6371  #kilometers\n",
      "    #r=3959 #miles\n",
      "    a= np.sin(d_2_rad*(phi_2-phi_1))**2+np.cos(d_2_rad*phi_1)*np.cos(d_2_rad*phi_2)*np.sin(d_2_rad*(lambda_2-lambda_1))**2\n",
      "    return 2*r*np.arctan(np.sqrt(a/(1-a)))\n",
      "\n",
      "\n",
      "\n",
      "#compute the mean haversine distance between -- not safe, make sure all array dimensions are the same\n",
      "def mean_haversine(phi_1s,lambda_1s,phi_2s,lambda_2s):\n",
      "\n",
      "    total=0\n",
      "    m=len(phi_1s)\n",
      "    for j in range(m):\n",
      "        #print haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
      "        total+=haversine(phi_1s[j],lambda_1s[j],phi_2s[j],lambda_2s[j])\n",
      "\n",
      "    return total/m\n",
      "        \n",
      "\n",
      "    \n",
      "\n",
      "# add the last distance delta from the POLYLINE field to main dataframe X and return it \n",
      "def lastDelta(X):\n",
      "    \n",
      "    deltas=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        if(len(entry)==0):  #this should not happen if length zero paths are excluded\n",
      "            deltas.append(-999)\n",
      "        elif(len(entry)==1):          \n",
      "            deltas.append(0)            \n",
      "        else:\n",
      "            last=entry[-1]\n",
      "            Sec2Last=entry[-2]\n",
      "            deltas.append(haversine(last[0],last[1],Sec2Last[0],Sec2Last[1]))\n",
      "            \n",
      "            \n",
      "    X['LDELTA']=deltas \n",
      "    \n",
      "    return X    \n",
      "\n",
      "\n",
      "# add the number of points in polyline\n",
      "def num_points(X):\n",
      "       \n",
      "    n_points=[]\n",
      "    \n",
      "    for j in range(len(X['POLYLINE'])):      \n",
      "        entry=X['POLYLINE'].values[j]\n",
      "        n_points.append(len(entry))\n",
      "                        \n",
      "    X['NPOINTS']=n_points \n",
      "    \n",
      "    return X\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Submission 4\n",
      "### hey let's try all the data this time\n",
      "### ... and maybe some cross validation wouldn't hurt\n",
      "\n",
      "Score: 2.85789"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Add cross-validation error and use entire data set in submission 4 (but in submission 3 only gets first 200000 events)\n",
      "#Try last_point, plus first point with random forest all data\n",
      "#requires a random truncation of the POLYLINE DATA (is there a \"best\" distribution ?)\n",
      "\n",
      "from sklearn import ensemble \n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=50\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    \n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    \n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=10))\n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "                \n",
      " \n",
      "\n",
      "            \n",
      "            \n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "    \n",
      "    \n",
      "\n",
      "        \n",
      "    \n",
      "zft = zipfile.ZipFile(\"../data/test.csv.zip\")\n",
      "test = pd.read_csv(zft.open('test.csv'), chunksize=100, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)}) \n",
      "\n",
      "first_chunk=True\n",
      "\n",
      "for chunk in test:\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    \n",
      "    chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
      "    \n",
      "    \n",
      "    m=len(clf)\n",
      "    predict=clf[0].predict(chunk.values[:,1:])\n",
      "    for j in range(1,m):  \n",
      "        predict +=clf[j].predict(chunk.values[:,1:])\n",
      "    \n",
      "    predict = predict/float(m)\n",
      "    \n",
      "    submit_df=pd.DataFrame({'TRIP_ID':chunk['TRIP_ID'], 'LONGITUDE':predict[:,1], \n",
      "                            'LATITUDE':predict[:,0]})\n",
      "    if first_chunk:\n",
      "        submit_df.to_csv('../data/4_forest.csv', mode='w', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False)\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        submit_df.to_csv('../data/4_forest.csv', mode='a', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False, header=False)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 1.40079409114\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is too small -- kagg\n",
      "# avg cv_error: 1.40079409114\n",
      "# np.array(cv_errors)[0:50].var() = 0.013143302328343818"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.array(cv_errors)[0:50].var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "0.013143302328343818"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#4B try submission 4 with more trees and compare CV errors\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "\n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=50\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    \n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    \n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15))\n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "zft = zipfile.ZipFile(\"../data/test.csv.zip\")\n",
      "test = pd.read_csv(zft.open('test.csv'), chunksize=100, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)}) \n",
      "\n",
      "first_chunk=True\n",
      "\n",
      "for chunk in test:\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    \n",
      "    chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON']]\n",
      "    \n",
      "    \n",
      "    m=len(clf)\n",
      "    predict=clf[0].predict(chunk.values[:,1:])\n",
      "    for j in range(1,m):  \n",
      "        predict +=clf[j].predict(chunk.values[:,1:])\n",
      "    \n",
      "    predict = predict/float(m)\n",
      "    \n",
      "    submit_df=pd.DataFrame({'TRIP_ID':chunk['TRIP_ID'], 'LONGITUDE':predict[:,1], \n",
      "                            'LATITUDE':predict[:,0]})\n",
      "    if first_chunk:\n",
      "        submit_df.to_csv('../data/4b_forest.csv', mode='w', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False)\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        submit_df.to_csv('../data/4b_forest.csv', mode='a', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False, header=False)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 1.38424255568\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Record CV errors from 4B for safe keeping -- about the same\n",
      "#\n",
      "#avg cv_error: 1.38424255568\n",
      "# np.array(cv_errors)[0:50].var()=0.013496034971940775"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.array(cv_errors)[0:50].var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "0.013496034971940775"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Throw in number of points\n",
      "### doesn't seem to matter much"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first and last point with number of points\n",
      "#\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "    \n",
      "    \n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=50\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)    \n",
      "    \n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "\n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=num_points(chunk)\n",
      "    \n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','NPOINTS']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15))\n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "        \n",
      "        \n",
      "        \n",
      "\n",
      "        \n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=num_points(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','NPOINTS']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "  \n",
      "        \n",
      "zft = zipfile.ZipFile(\"../data/test.csv.zip\")\n",
      "test = pd.read_csv(zft.open('test.csv'), chunksize=100, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)}) \n",
      "\n",
      "first_chunk=True\n",
      "\n",
      "for chunk in test:\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=num_points(chunk)\n",
      "    \n",
      "    chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON', 'NPOINTS']]\n",
      "    \n",
      "    \n",
      "    m=len(clf)\n",
      "    predict=clf[0].predict(chunk.values[:,1:])\n",
      "    for j in range(1,m):  \n",
      "        predict +=clf[j].predict(chunk.values[:,1:])\n",
      "    \n",
      "    predict = predict/float(m)\n",
      "    \n",
      "    submit_df=pd.DataFrame({'TRIP_ID':chunk['TRIP_ID'], 'LONGITUDE':predict[:,1], \n",
      "                            'LATITUDE':predict[:,0]})\n",
      "    if first_chunk:\n",
      "        submit_df.to_csv('../data/5_forest.csv', mode='w', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False)\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        submit_df.to_csv('../data/5_forest.csv', mode='a', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False, header=False)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 1.39809268198\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.array(cv_errors)[0:50].var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "0.015657487057966456"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#avg cv_error: 1.39809268198\n",
      "#np.array(cv_errors)[0:50].var() = 0.015657487057966456\n",
      "#\n",
      "# not any better -- actually a little worse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sumbission 5\n",
      "### cv_error (and intuition) suggests that lastDelta should help, leaderboard says otherwise\n",
      "\n",
      "file: 6_forest.csv\n",
      "\n",
      "Score: 2.92342"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first and last point with number of points\n",
      "#\n",
      "#\n",
      "\n",
      "from sklearn import ensemble \n",
      "\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "    \n",
      "    \n",
      "target=pd.DataFrame()\n",
      "train_predictors=pd.DataFrame()\n",
      "\n",
      "first_chunk=True\n",
      "k=0\n",
      "cutoff=50\n",
      "clf=[] \n",
      "\n",
      "for chunk in train_parser:\n",
      "\n",
      "    chunk=rem_empty_polyline(chunk)    \n",
      "    chunk=rem_missing(chunk)\n",
      "    chunk=lat_long_last(chunk)    \n",
      "    \n",
      "    if(first_chunk):\n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "    else:\n",
      "        target=pd.concat([target,chunk[['LAST_LAT','LAST_LON']]])\n",
      "        \n",
      "\n",
      "    #\n",
      "    # MODIFY by randomly truncating POLYLINE for training\n",
      "    #  and recompute predictor features\n",
      "    #\n",
      "    chunk=trunc_polyline(chunk)\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=lastDelta(chunk)\n",
      "    \n",
      "    chunk=chunk[['LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDELTA']]\n",
      "\n",
      "    if(first_chunk):\n",
      "        train_predictors=chunk\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        train_predictors=pd.concat([train_predictors,chunk])\n",
      "    \n",
      "    k+=1\n",
      "    if(k==cutoff):\n",
      "        clf.append(ensemble.RandomForestRegressor(n_estimators=15))\n",
      "        m=len(clf)-1\n",
      "        clf[m].fit(train_predictors.values,target.values)\n",
      "        first_chunk=True\n",
      "        k=0\n",
      "\n",
      "        \n",
      "        \n",
      "        \n",
      "\n",
      "        \n",
      "## \n",
      "## Compute ~50/50 CV error\n",
      "##\n",
      "l=0\n",
      "m=len(clf)\n",
      "cut=round(float(m)/2)\n",
      "cv_errors=[]\n",
      "cv_chunks=50\n",
      "\n",
      "# re-initialize the train data parser\n",
      "train_parser=load_train(1000)\n",
      "\n",
      "\n",
      "if(m<2):\n",
      "    print \"only one segment, cannot compute CV error\"\n",
      "else:\n",
      "    \n",
      "    for chunk in train_parser:\n",
      "        chunk=rem_empty_polyline(chunk)    \n",
      "        chunk=rem_missing(chunk)\n",
      "        chunk=lat_long_last(chunk)\n",
      "        chunk=lat_long_first(chunk)\n",
      "        chunk=lastDelta(chunk)\n",
      "        \n",
      "        target=chunk[['LAST_LAT','LAST_LON']]\n",
      "        chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON','LDELTA']]\n",
      "        \n",
      "        chunks_models=cut*cutoff\n",
      "        if (l>=chunks_models) and (l<chunks_models+cv_chunks) :\n",
      "         \n",
      "            predict=0\n",
      "            for j in range(1,int(cut)):  \n",
      "                predict +=clf[j].predict(chunk.values[:,1:])\n",
      "            \n",
      "            predict = predict/float(cut-1)\n",
      "                \n",
      "            mean_dist=mean_haversine( predict[:,0],predict[:,1],target['LAST_LAT'].values,target['LAST_LON'].values)\n",
      "            cv_errors.append(mean_dist)\n",
      "        l+=1\n",
      "        \n",
      "    #print \"cv_errors: \" + str(cv_errors)\n",
      "    print \"avg cv_error: \" + str(np.array(cv_errors).mean())\n",
      "\n",
      "#\n",
      "# END of CV calculation\n",
      "#\n",
      "        \n",
      "        \n",
      "        \n",
      "  \n",
      "        \n",
      "zft = zipfile.ZipFile(\"../data/test.csv.zip\")\n",
      "test = pd.read_csv(zft.open('test.csv'), chunksize=100, iterator=True, converters={'POLYLINE': lambda x: json.loads(x)}) \n",
      "\n",
      "first_chunk=True\n",
      "\n",
      "for chunk in test:\n",
      "    chunk=lat_long_last(chunk)\n",
      "    chunk=lat_long_first(chunk)\n",
      "    chunk=lastDelta(chunk)\n",
      "    \n",
      "    chunk=chunk[['TRIP_ID','LAST_LAT','LAST_LON','FIRST_LAT','FIRST_LON', 'LDELTA']]\n",
      "    \n",
      "    \n",
      "    m=len(clf)\n",
      "    predict=clf[0].predict(chunk.values[:,1:])\n",
      "    for j in range(1,m):  \n",
      "        predict +=clf[j].predict(chunk.values[:,1:])\n",
      "    \n",
      "    predict = predict/float(m)\n",
      "    \n",
      "    submit_df=pd.DataFrame({'TRIP_ID':chunk['TRIP_ID'], 'LONGITUDE':predict[:,1], \n",
      "                            'LATITUDE':predict[:,0]})\n",
      "    if first_chunk:\n",
      "        submit_df.to_csv('../data/6_forest.csv', mode='w', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False)\n",
      "        first_chunk=False\n",
      "    else:\n",
      "        submit_df.to_csv('../data/6_forest.csv', mode='a', columns=['TRIP_ID','LATITUDE','LONGITUDE'], index=False, header=False)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg cv_error: 1.20507877642\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}